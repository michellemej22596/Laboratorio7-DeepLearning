{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf648516",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "### Michelle Mejía, Nelson García Bravatti, Joaquín Puente y Silvia Illescas\n",
    "\n",
    "#### Task 2 - Teoría\n",
    "Defina en qué consiste y en qué clase de problemas se pueden usar cada uno de los siguientes acercamientos en\n",
    "Deep Reinforcement Learning:\n",
    "\n",
    "\n",
    "1. Proximal Policy Optimization\n",
    "\n",
    "    Es un algoritmo que entrena la política de forma estable, limitando los cambios muy bruscos con un sistema de clipping. Se usa en juegos, simulaciones o robots donde es importante que el aprendizaje sea seguro y no se descontrole. Además, PPO pertenece a la familia policy gradient on-policy y sustituye la restricción KL explícita de TRPO por una función objetivo “recortada” que evita pasos de actualización excesivos sin complicar la optimización. En la práctica se combina con ventajas generalizadas (GAE) y entropy bonus para equilibrar exploración y explotación, y funciona tanto en acciones discretas como continuas. Por su simplicidad y estabilidad suele ser un sólido baseline en Atari y MuJoCo, aunque, al ser on-policy, requiere más muestras que enfoques off-policy.\n",
    "\n",
    "\n",
    "\n",
    "2. Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "    Es un método actor-crítico pensado para espacios de acción continuos. El actor elige la acción y el crítico evalúa si fue buena. Se aplica en brazos robóticos, autos autónomos o simulaciones físicas donde las acciones no son discretas. En detalle, DDPG es off-policy y determinista: reusa experiencia con replay buffer, estabiliza el entrenamiento con redes objetivo y suele inyectar ruido, por ejemplo: Ornstein-Uhlenbeck; en la acción para explorar. Es eficiente en muestreo y adecuado para control fino de alta dimensión, pero puede ser frágil sin buen tuning, lo que motivó mejoras como TD3 y SAC que reducen el sobreajuste al valor y la sobreestimación de Q con técnicas de target policy smoothing y entropía máxima.\n",
    "\n",
    "\n",
    "\n",
    "3. Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "    Busca optimizar la política asegurando que cada cambio se mantenga dentro de una “zona de confianza” o trust region. Esto evita actualizaciones demasiado grandes que pueden dañar el aprendizaje. Se usa en problemas complejos como locomoción 3D o control de robots. TRPO es on-policy y formula la mejora de política con una cota de divergencia KL que garantiza, de forma aproximada, mejoras monótonas; implementa el paso mediante conjugate gradient y line search. Sus garantías y estabilidad lo hicieron un hito para control continuo, aunque su complejidad computacional y de implementación condujo a PPO, que busca efectos similares con un objetivo recortado más simple.\n",
    "\n",
    "\n",
    "\n",
    "4. Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "\n",
    "    Hace que varios agentes aprendan en paralelo, cada uno con su propia copia del entorno. Esto acelera el entrenamiento y mejora la exploración. Se aplica en videojuegos, navegación autónoma y entornos grandes que aprovechan varios núcleos de CPU. Concretamente, A3C usa múltiples workers asíncronos que actualizan parámetros compartidos, emplea retornos n-step y la función de ventaja para reducir varianza, y prescinde del replay buffer, lo que facilita el entrenamiento en CPU pura. Aporta diversidad de trayectorias y velocidad, aunque suele ser menos eficiente en muestras que métodos off-policy; su versión sincrónica (A2C) mitiga el ruido de gradientes coordinando las actualizaciones.\n",
    "\n",
    "\n",
    "Referencias:\n",
    "\n",
    "Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2015, September 9). Continuous control with deep reinforcement learning. arXiv.org. https://arxiv.org/abs/1509.02971\n",
    "\n",
    "Schulman, J., Levine, S., Moritz, P., Jordan, M. I., & Abbeel, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (pp. 1889–1897). PMLR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa46cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
