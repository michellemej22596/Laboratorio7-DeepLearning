{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf648516",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "### Michelle Mejía 22596 y Silvia Illescas 22376\n",
    "\n",
    "#### Task 2 - Teoría\n",
    "Defina en qué consiste y en qué clase de problemas se pueden usar cada uno de los siguientes acercamientos en\n",
    "Deep Reinforcement Learning:\n",
    "\n",
    "\n",
    "1. Proximal Policy Optimization\n",
    "\n",
    "    Es un algoritmo que entrena la política de forma estable, limitando los cambios muy bruscos con un sistema de *clipping*. Se usa en juegos, simulaciones o robots donde es importante que el aprendizaje sea seguro y no se descontrole.\n",
    "\n",
    "\n",
    "\n",
    "2. Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "    Es un método *actor-crítico* pensado para espacios de acción continuos. El actor elige la acción y el crítico evalúa si fue buena. Se aplica en brazos robóticos, autos autónomos o simulaciones físicas donde las acciones no son discretas.\n",
    "\n",
    "\n",
    "\n",
    "3. Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "    Busca optimizar la política asegurando que cada cambio se mantenga dentro de una “zona de confianza” o *trust region*. Esto evita actualizaciones demasiado grandes que pueden dañar el aprendizaje. Se usa en problemas complejos como locomoción 3D o control de robots.\n",
    "\n",
    "\n",
    "\n",
    "4. Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "\n",
    "    Hace que varios agentes aprendan en paralelo, cada uno con su propia copia del entorno. Esto acelera el entrenamiento y mejora la exploración. Se aplica en videojuegos, navegación autónoma y entornos grandes que aprovechan varios núcleos de CPU.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
